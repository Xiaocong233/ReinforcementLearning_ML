{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epsilon_greedy_tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOov/k3uGQs6UygDqwmpJ9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaocong233/ReinforcementLearning_ML/blob/master/epsilon_greedy_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8uS_JAfWGOn",
        "colab_type": "text"
      },
      "source": [
        "# **Reinforcement Learning Tutorial in Python**\n",
        "###### Created by **Xiaocong Yan** for [StartOnAI](https://startonai.com/)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA8EDDN0Xtlv",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JxN8ixlZRc1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70m6nTsAaXFD",
        "colab_type": "text"
      },
      "source": [
        "- What is Reinforcement Learning?\n",
        "  - RL is a subfield in machine learning, it particularly focuses on training AI agents to behave in a certain way by learning directly from its surrounding environment\n",
        "  - Essentially, we are training the agent to choose the optimal action (a) given a state (s) from the environment that will maxmimizes an engineered reward (r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5hf1YrgCzC",
        "colab_type": "text"
      },
      "source": [
        "- RL Applications\n",
        "  - gameplaying AI\n",
        "    - AlphaGo\n",
        "\n",
        "    <img src=\"https://cdn.geekwire.com/wp-content/uploads/2016/03/160312-go-630x353.jpg\" alt=\"alt text\" width=\"500\" height=\"300\">\n",
        "\n",
        "    - AlphaStar\n",
        "\n",
        "    <img src=\"https://www.version2.dk/sites/v2/files/topillustration/2019/01/alphastarscreenshot.png\" alt=\"alt text\" width=\"600\" height=\"337\">\n",
        "  \n",
        "  - agent in simulation learning to walk\n",
        "  \n",
        "  <img src=\"https://nav74neet.github.io/media/blog/walking.png\" alt=\"alt text\"  width='740' height='300'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiG2yaOoYwAE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Explore-exploit dilemma and Multi-Armed Bandit Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYuMPoL-LJdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class Bandit:\n",
        "  def __init__(self, p):\n",
        "    self.p = p # the winning rate\n",
        "    self.p_estimate = 0. # estimation of the winning rate, intialized to 0\n",
        "    self.N = 0. # number of samples collected\n",
        "\n",
        "  def pull(self):\n",
        "    # draw a random probability p and check if won according to the winning rate\n",
        "    return np.random.random() < self.p\n",
        "\n",
        "  def update(self, x):\n",
        "    # increment numbers of samples collected\n",
        "    self.N += 1.\n",
        "    # calculate the new p hat from the previous p hat and the newly obtained value\n",
        "    self.p_estimate = ((self.N - 1) * self.p_estimate + x) / self.N\n",
        "\n",
        "def run_experiment(bandits_probs_list, epsilon, N):\n",
        "  # create a list of bandit objects according to their probabilities of win rate\n",
        "  bandits = [Bandit(p) for p in bandits_probs_list]\n",
        "  \n",
        "  # initialize variables\n",
        "  rewards = np.zeros(N)\n",
        "  num_times_explored = 0\n",
        "  num_times_exploited = 0\n",
        "  num_optimal = 0\n",
        "\n",
        "  # print out the true optimal bandit index\n",
        "  optimal_j = np.argmax([b.p for b in bandits])\n",
        "  print('optimal j:', optimal_j)\n",
        "\n",
        "  for i in range(N):\n",
        "    # use epsilon_greedy to select the next bandit\n",
        "    if np.random.random() < epsilon:\n",
        "      num_times_explored += 1\n",
        "      j = np.random.randint(len(bandits))\n",
        "    else:\n",
        "      num_times_exploited += 1\n",
        "      j = np.argmax([b.p_estimate for b in bandits])\n",
        "    \n",
        "    if j == optimal_j:\n",
        "      num_optimal += 1\n",
        "\n",
        "    # pull the arm for the bandit selected\n",
        "    x = bandits[j].pull()\n",
        "\n",
        "    # update rewards log\n",
        "    rewards[i] = x\n",
        "    bandits[j].update(x)     \n",
        "  \n",
        "  # print mean estimates for each bandit\n",
        "  for i, b in enumerate(bandits):\n",
        "    print(f'bandit{i + 1} estimate win-rate: {round(b.p_estimate, 3)} | true win_rate: {b.p}')\n",
        "\n",
        "  # print total reward\n",
        "  print()\n",
        "  print('total reward:', rewards.sum())\n",
        "  print('overall win-rate:', rewards.sum() / N)\n",
        "  print('explore count:', num_times_explored)\n",
        "  print('exploit count:', num_times_exploited)\n",
        "  print('optimal selection count:', num_optimal)\n",
        "\n",
        "  # plot the results\n",
        "  cumulative_rewards = np.cumsum(rewards)\n",
        "  win_rates = cumulative_rewards / (np.arange(N) + 1)\n",
        "  plt.plot(win_rates)\n",
        "  plt.plot(np.ones(N) * np.max(bandits_probs_list))\n",
        "  plt.title('cumulative win-rate over time')\n",
        "  plt.xlabel('number of trials')\n",
        "  plt.ylabel('win-rate')\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # simulate a multi-armed bandit problem with 5 machines with win-rates 0, 0.25, 0.5, 0.75 and 1\n",
        "  # default random selection to happen 10% of the time, thus selecting epsilon of 0.1\n",
        "  # default to 10000 trials\n",
        "  run_experiment([0, 0.25, 0.5, 0.75, 1], 0.1, 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qJBTOpXM7u",
        "colab_type": "text"
      },
      "source": [
        "## Sources:"
      ]
    }
  ]
}