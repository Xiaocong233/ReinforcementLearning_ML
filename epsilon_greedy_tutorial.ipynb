{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epsilon_greedy_tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzhPIU0b/PYAXozUXIjYFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaocong233/ReinforcementLearning_ML/blob/master/epsilon_greedy_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8uS_JAfWGOn",
        "colab_type": "text"
      },
      "source": [
        "# **Reinforcement Learning Tutorial in Python**\n",
        "###### Created by **Xiaocong Yan** for [StartOnAI](https://startonai.com/)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA8EDDN0Xtlv",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JxN8ixlZRc1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70m6nTsAaXFD",
        "colab_type": "text"
      },
      "source": [
        "- What is Reinforcement Learning?\n",
        "  - RL is a subfield in machine learning, it particularly focuses on training AI agents to behave in a certain way by learning directly from its surrounding environment\n",
        "  - Essentially, we are training the agent to choose the optimal action (`a`) given a state (`s`) from the environment that will maxmimizes an engineered reward (`r`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5hf1YrgCzC",
        "colab_type": "text"
      },
      "source": [
        "- RL Applications\n",
        "  - gameplaying AI\n",
        "    - AlphaGo\n",
        "\n",
        "    <img src=\"https://cdn.geekwire.com/wp-content/uploads/2016/03/160312-go-630x353.jpg\" alt=\"alt text\" width=\"500\" height=\"300\">\n",
        "\n",
        "    - AlphaStar\n",
        "\n",
        "    <img src=\"https://www.version2.dk/sites/v2/files/topillustration/2019/01/alphastarscreenshot.png\" alt=\"alt text\" width=\"600\" height=\"337\">\n",
        "  \n",
        "  - agent in simulation learning to walk\n",
        "  \n",
        "  <img src=\"https://nav74neet.github.io/media/blog/walking.png\" alt=\"alt text\"  width='600' height='250'>\n",
        "\n",
        "  - robots learning to walk\n",
        "\n",
        "    <img src=\"https://www.researchgate.net/profile/Pieter_Jonker/publication/236015074/figure/fig1/AS:299857928572950@1448503109999/a-LEO-a-2D-walking-robot-suitable-for-on-line-Reinforcement-Learning-8-b-Simplest.png\" alt=\"alt text\"  width='340' height='255'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiG2yaOoYwAE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Explore-exploit dilemma and Multi-Armed Bandit Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nUdqjmn4fOf",
        "colab_type": "text"
      },
      "source": [
        "- A fundamental intuition of RL lies within the balancing of explore and exploit\n",
        "- Example: casino multi-armed bandits (slot machines)\n",
        "  ![alt text](https://miro.medium.com/max/1250/1*7axVBpiVF4VQCxxP1UNcnw.png)\n",
        "    - suppose we have three bandits with their own unique probablities of winning the jackpot\n",
        "      - in the start, we have no ideas what the probabilities are\n",
        "      - we want to start playing more: \"explore\" to find the bandit with the highest winning rate as quickly as possible such that we can then \"exploit\" it by playing solely on it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jr07JV7C95k",
        "colab_type": "text"
      },
      "source": [
        "### 2A. Greedy\n",
        "  - drawing upon an updating sample proportion `p_hat` of each bandit (current number of wins / times played on the bandit), a basic greedy algorithm, as its name suggests, will only choose the bandit with the highest current sample proportion\n",
        "  - problem: `p_hat` may be drastically different than the real probability, especially in the beginning where we don't have many samples or times played yet\n",
        "    - if we draw two bandits and one return a win and the other a loss, updating `p_hat` will result in 1 for the victorious bandit and 0 for the losing bandit.\n",
        "      - we will never again choose the losing bandit ever again using greedy algorithm since no matter how many times we update the probability for the victorious bandit, it will never reach below 0\n",
        "      - we will be missing out on exploring the losing bandit at all and choose to solely exploit the winning bandit, when in reality, the unlucky losing bandit may have a much higher real winning probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMz8HXo5HKtI",
        "colab_type": "text"
      },
      "source": [
        "### 2B. Epsilon Greedy\n",
        "  - solution to the greedy problem: adding a chance in each draw, parameterized by variable `epsilon`, where we will choose randomly from all the existing bandits, regardless of their sample proportions\n",
        "  - thus it is created a fundamental algorithm behind RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYuMPoL-LJdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# creating the blueprint for a bandit slot machine\n",
        "class Bandit:\n",
        "  def __init__(self, p):\n",
        "    self.p = p # the winning rate\n",
        "    self.p_hat = 0. # sample proportion, or the estimation for the winning rate, intialized to 0\n",
        "    self.n = 0. # number of samples collected\n",
        "\n",
        "  def pull(self):\n",
        "    # draw a random probability p and check if won according to the winning rate\n",
        "    return np.random.random() < self.p\n",
        "\n",
        "  def update(self, x):\n",
        "    # increment numbers of samples collected\n",
        "    self.n += 1.\n",
        "    # calculate the new p hat from the previous p hat and the newly obtained value\n",
        "    self.p_hat = ((self.n - 1) * self.p_hat + x) / self.n\n",
        "\n",
        "def run_simulation(bandits_probs_list, epsilon, n):\n",
        "  # create a list of bandit objects according to their probabilities of win rate\n",
        "  bandits = [Bandit(p) for p in bandits_probs_list]\n",
        "  \n",
        "  # initialize variables\n",
        "  rewards = np.zeros(n)\n",
        "  num_times_explored = 0\n",
        "  num_times_exploited = 0\n",
        "  num_optimal = 0\n",
        "\n",
        "  # print out the true optimal bandit index\n",
        "  optimal_j = np.argmax([b.p for b in bandits])\n",
        "  print('optimal j:', optimal_j)\n",
        "\n",
        "  for i in range(n):\n",
        "    # use epsilon_greedy to select the next bandit\n",
        "    if np.random.random() < epsilon:\n",
        "      num_times_explored += 1\n",
        "      j = np.random.randint(len(bandits))\n",
        "    else:\n",
        "      num_times_exploited += 1\n",
        "      j = np.argmax([b.p_hat for b in bandits])\n",
        "    \n",
        "    if j == optimal_j:\n",
        "      num_optimal += 1\n",
        "\n",
        "    # pull the arm for the bandit selected\n",
        "    x = bandits[j].pull()\n",
        "\n",
        "    # update rewards log\n",
        "    rewards[i] = x\n",
        "    bandits[j].update(x)     \n",
        "  \n",
        "  # print mean estimates for each bandit\n",
        "  for i, b in enumerate(bandits):\n",
        "    print(f'bandit{i + 1} estimate win-rate: {round(b.p_hat, 3)} | true win_rate: {b.p}')\n",
        "\n",
        "  # print total reward\n",
        "  print()\n",
        "  print('total reward:', rewards.sum())\n",
        "  print('overall win-rate:', rewards.sum() / n)\n",
        "  print('explore count:', num_times_explored)\n",
        "  print('exploit count:', num_times_exploited)\n",
        "  print('optimal selection count:', num_optimal)\n",
        "\n",
        "  # plot the results\n",
        "  cumulative_rewards = np.cumsum(rewards)\n",
        "  win_rates = cumulative_rewards / (np.arange(n) + 1)\n",
        "  plt.plot(win_rates)\n",
        "  plt.plot(np.ones(n) * np.max(bandits_probs_list))\n",
        "  plt.title('cumulative win-rate over time')\n",
        "  plt.xlabel('number of trials')\n",
        "  plt.ylabel('win-rate')\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # simulate a multi-armed bandit problem with 5 machines with win-rates 0, 0.25, 0.5, 0.75 and 1\n",
        "  # default random selection to happen 10% of the time, thus selecting epsilon of 0.1\n",
        "  # default to 10000 trials\n",
        "  run_simulation([0, 0.25, 0.5, 0.75, 1], 0.1, 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qJBTOpXM7u",
        "colab_type": "text"
      },
      "source": [
        "## Sources:"
      ]
    }
  ]
}